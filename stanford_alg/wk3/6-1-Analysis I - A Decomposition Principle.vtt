WEBVTT

1
00:00:00.150 --> 00:00:03.300
So this is the first video of three
in which we'll mathematically analyze

2
00:00:03.300 --> 00:00:06.450
the running time of the randomized
implementation of quick sort.

3
00:00:06.450 --> 00:00:09.900
So in particular we're going to prove that
the average running time of quick sort

4
00:00:09.900 --> 00:00:11.490
is big O of n log n.

5
00:00:11.490 --> 00:00:14.190
Now this is the first randomized algorithm
that we've seen in the course and

6
00:00:14.190 --> 00:00:16.980
therefore in its analysis will be the
first time that we're going to need any

7
00:00:16.980 --> 00:00:18.640
kind of probability theory.

8
00:00:18.640 --> 00:00:21.970
So let me just explain upfront what
I'm going to expect you to know.

9
00:00:21.970 --> 00:00:23.450
In the following analysis.

10
00:00:23.450 --> 00:00:26.120
Basically, I need you to know
the first few ingredients

11
00:00:26.120 --> 00:00:27.910
of discrete probability theory.

12
00:00:27.910 --> 00:00:29.890
So I need you to know about sample spaces,

13
00:00:29.890 --> 00:00:32.090
that is how to model all of
the different things that could happen,

14
00:00:32.090 --> 00:00:34.930
all of the ways that random
choices could resolve themselves.

15
00:00:34.930 --> 00:00:38.400
I need you to know about random variables,
functions on sample spaces,

16
00:00:38.400 --> 00:00:39.970
which take on real values.

17
00:00:39.970 --> 00:00:44.810
I need you to know about expectations that
is average values of random variables and

18
00:00:44.810 --> 00:00:48.080
very simple but very key propriety
we're going to need in the analysis

19
00:00:48.080 --> 00:00:50.740
of quick sort is linearity of expectation.

20
00:00:50.740 --> 00:00:54.150
So if you haven't seen this before or
if you're too rusty

21
00:00:54.150 --> 00:00:57.390
definitely you should review this
stuff before you watch this video.

22
00:00:57.390 --> 00:01:00.640
Some places you can go to get that
necessary review you can look at

23
00:01:00.640 --> 00:01:02.900
the probability review part one video.

24
00:01:02.900 --> 00:01:05.240
That's up on the course's website.

25
00:01:05.240 --> 00:01:08.220
If you'd prefer to read something, like
I said at the beginning of the course,

26
00:01:08.220 --> 00:01:12.220
I recommend the free online
lecture notes by Eric Lehman and

27
00:01:12.220 --> 00:01:14.790
Tom Leighton, Mathematics for
Computer Science.

28
00:01:14.790 --> 00:01:17.050
That covers everything we'll need to know,
plus much, much more.

29
00:01:17.050 --> 00:01:20.990
There's also a Wikibook on Discrete
Probability, which is a perfectly fine,

30
00:01:20.990 --> 00:01:24.600
obviously, free source in which you
can learn the necessary material.

31
00:01:24.600 --> 00:01:26.440
Okay?
So after you've got that

32
00:01:26.440 --> 00:01:29.250
sort of fresh in your mind, then you're
ready to watch the rest of this video.

33
00:01:29.250 --> 00:01:30.190
And in particular,

34
00:01:30.190 --> 00:01:34.450
we're ready to prove the following
theorems stated in the previous video.

35
00:01:34.450 --> 00:01:37.740
So the quick sort algorithm with
a randomized implementation, that is we're

36
00:01:37.740 --> 00:01:41.960
in every single recursive subcall,
you pick a pivot uniformly at random.

37
00:01:41.960 --> 00:01:43.690
We stated the following assertion.

38
00:01:43.690 --> 00:01:47.270
But for every single input, so for
a worst case input array of length n,

39
00:01:47.270 --> 00:01:52.541
the average running time of QuickSort
with random pivots is O(n log n).

40
00:01:52.541 --> 00:01:54.750
And again,
to be clear where the randomness is,

41
00:01:54.750 --> 00:01:56.300
the randomness is not in the data.

42
00:01:56.300 --> 00:01:58.030
We make no assumptions about the data.

43
00:01:58.030 --> 00:01:59.620
As per our guiding principles.

44
00:01:59.620 --> 00:02:02.530
No matter what the input array is,
averaging only

45
00:02:02.530 --> 00:02:07.030
over the randomness in our own code,
the randomness internal to our algorithm.

46
00:02:07.030 --> 00:02:08.910
We get a running time of n log n.

47
00:02:08.910 --> 00:02:12.510
We saw in the past that the best case
behavior of QuickSort is n log n.

48
00:02:12.510 --> 00:02:14.700
Its worst case behavior is n squared.

49
00:02:14.700 --> 00:02:17.860
So this theorem is asserting that
no matter what the input array is,

50
00:02:17.860 --> 00:02:22.240
the typical behavior of QuickSort is
far closer to the best case behavior

51
00:02:22.240 --> 00:02:23.820
than it is to the worst case behavior.

52
00:02:23.820 --> 00:02:25.730
Okay.
So that's what we're going to prove in

53
00:02:25.730 --> 00:02:26.580
the next few videos.

54
00:02:26.580 --> 00:02:27.644
So let's go ahead and get started.

55
00:02:30.019 --> 00:02:33.543
So first I'm going to set up the necessary
notation and be clear about what exactly

56
00:02:33.543 --> 00:02:37.078
is the sample space, what is the random
variable that we care about, and so on.

57
00:02:39.383 --> 00:02:41.855
So we're going to fix
an arbitrary array of length N.

58
00:02:41.855 --> 00:02:44.230
That's going to be the input
to the quick sort algorithm.

59
00:02:44.230 --> 00:02:48.215
[SOUND].

60
00:02:48.215 --> 00:02:51.534
And we'll be working with this fixed but
arbitrary input array for

61
00:02:51.534 --> 00:02:53.145
the remainder of the analysis.

62
00:02:53.145 --> 00:02:56.095
Okay.
So just fix a single input in your mind.

63
00:02:56.095 --> 00:02:57.539
Now, what's the relevant sample space?

64
00:02:59.480 --> 00:03:00.970
Well, recall what a sample space is.

65
00:03:00.970 --> 00:03:04.620
It's just all the possible outcomes
of the randomness in the world.

66
00:03:04.620 --> 00:03:07.370
So it's all the distinct
things that could happen.

67
00:03:07.370 --> 00:03:10.120
Now here,
the randomness is of our own devising.

68
00:03:10.120 --> 00:03:13.570
It's just the random pivot sequences,
the random pivots chosen by QuickSort.

69
00:03:13.570 --> 00:03:18.178
So omega is just the set of all possible
random pivots the QuickSort could choose.

70
00:03:21.219 --> 00:03:23.794
Now the whole point of
this theorem proving

71
00:03:23.794 --> 00:03:26.512
that the average running
time of quick sort is

72
00:03:26.512 --> 00:03:31.560
small boils down to computing the
expectation of a single random variable.

73
00:03:31.560 --> 00:03:33.824
So here's the random variable
we're going to care about.

74
00:03:37.189 --> 00:03:42.230
For a given pivot sequence remember that
random variables are real value functions.

75
00:03:42.230 --> 00:03:43.380
Defined on the sample space.

76
00:03:43.380 --> 00:03:48.680
So for a given point in the sample space
or pivot sequence sigma, we're going

77
00:03:48.680 --> 00:03:53.980
to define C of sigma as the number of
comparisons that quick sort makes.

78
00:03:53.980 --> 00:03:55.180
Where by comparison,

79
00:03:55.180 --> 00:03:57.390
I don't mean something like with
an array index in a for-loop.

80
00:03:57.390 --> 00:03:59.230
That's not what I mean by comparison.

81
00:03:59.230 --> 00:04:02.601
I mean a comparison between two
different entries of the input array,

82
00:04:02.601 --> 00:04:06.486
by comparing the third entry in the array
against the seventh entry in the array,

83
00:04:06.486 --> 00:04:09.530
to see whether the third entry or
the seventh entry is smaller.

84
00:04:13.137 --> 00:04:15.957
Notice that this is indeed
a random variable that

85
00:04:15.957 --> 00:04:20.930
is given knowledge of the pivot sequence
sigma, the choices of all pivots.

86
00:04:20.930 --> 00:04:24.180
You can think of quick sort at that
point as just a deterministic algorithm

87
00:04:24.180 --> 00:04:27.570
with all of the pivot choices
pre-determined, and so a deterministic

88
00:04:27.570 --> 00:04:31.770
version of QuickSort make some
deterministic member of comparisons so for

89
00:04:31.770 --> 00:04:34.720
giving pivot sequence sigma,
we're just calling C of sigma

90
00:04:34.720 --> 00:04:38.699
to be however many comparisons it
makes given those choices of pivots.

91
00:04:41.150 --> 00:04:44.630
Now with the theorem I stated is not about
the number of comparisons of QuickSort but

92
00:04:44.630 --> 00:04:47.740
rather about the running time
of QuickSort, but really to

93
00:04:47.740 --> 00:04:51.310
think about it kind of the only real
work that the QuickSort algorithm does,

94
00:04:51.310 --> 00:04:55.500
is make comparisons between pairs
of elements in the input array.

95
00:04:55.500 --> 00:04:57.570
The axis is a little bit
of other book keeping but

96
00:04:57.570 --> 00:04:59.830
that's all noise that second over stuff.

97
00:04:59.830 --> 00:05:03.494
All QuickSort really does is compare
between pairs of elements in

98
00:05:03.494 --> 00:05:04.386
the input array.

99
00:05:06.509 --> 00:05:10.272
And if you want to know what I mean
by that a little more formally,

100
00:05:10.272 --> 00:05:14.176
dominated by comparisons,
I mean that there exists a constant C so

101
00:05:14.176 --> 00:05:18.918
that the total number of operations of any
type that QuickSort executes is at most

102
00:05:18.918 --> 00:05:22.367
a constant factor larger than
the number of comparisons.

103
00:05:24.130 --> 00:05:28.580
So lets say that by RT, I mean the number
of primitive operations of any form,

104
00:05:28.580 --> 00:05:30.250
that QuickSort uses.

105
00:05:30.250 --> 00:05:35.880
And for every previd sequence, sigma,
the total number of operations,

106
00:05:35.880 --> 00:05:39.710
is no more than a constant times
the total number of comparisons.

107
00:05:40.770 --> 00:05:43.380
And if you want a proof of this
it's not that interesting so

108
00:05:43.380 --> 00:05:44.420
I'm not going to talk about it here.

109
00:05:44.420 --> 00:05:49.050
But in the notes posted on the website
there is a sketch of why this is true.

110
00:05:49.050 --> 00:05:51.010
How you can formally argue
that there isn't much work

111
00:05:51.010 --> 00:05:52.240
beyond just the comparisons.

112
00:05:52.240 --> 00:05:55.540
But I hope most of you find
that to be pretty intuitive.

113
00:05:55.540 --> 00:05:58.240
So given this,
given that the running time that QuickSort

114
00:05:58.240 --> 00:06:00.050
boils down just to
the number of comparisons.

115
00:06:00.050 --> 00:06:02.350
We want to prove the running
time is n log n.

116
00:06:02.350 --> 00:06:05.770
All we gotta do, quote unquote, all we
have to do this proves that the average

117
00:06:05.770 --> 00:06:08.520
number of comparisons
the QuickSort mix is all nlogn.

118
00:06:08.520 --> 00:06:09.950
And that's what we're going to do.

119
00:06:09.950 --> 00:06:13.828
That's what the rest of
these lecture is all about.

120
00:06:13.828 --> 00:06:15.000
So that's what we got to prove.

121
00:06:15.000 --> 00:06:19.599
We got to prove the expectation of
this random variable C which counts up

122
00:06:19.599 --> 00:06:24.966
the number of comparisons QuickSort mix is
for arbitrary input array of link n bound

123
00:06:24.966 --> 00:06:30.076
by big O of nlogn So

124
00:06:30.076 --> 00:06:33.660
the high order bit of this lecture
is a decomposition principle.

125
00:06:33.660 --> 00:06:36.820
We've identified this random variable,
C, the number of comparisons and

126
00:06:36.820 --> 00:06:38.100
it's exactly what we care about.

127
00:06:38.100 --> 00:06:40.220
It governs the average
running time of QuickSort.

128
00:06:40.220 --> 00:06:42.230
The problem is, it's quite complicated.

129
00:06:42.230 --> 00:06:45.240
It's very hard to understand
what this capital C is,

130
00:06:45.240 --> 00:06:47.940
it's fluctuating between nlogn and
then squared.

131
00:06:47.940 --> 00:06:52.040
And it's hard to know how
to get a handle on it.

132
00:06:52.040 --> 00:06:56.309
So how are we going to go about proving
this assertion, that the expectant number

133
00:06:56.309 --> 00:07:00.076
of comparisons that QuickSort makes,
is on average just O of nlogn.

134
00:07:00.076 --> 00:07:03.380
At this point we've actually have a fair
amount of experience with divide and

135
00:07:03.380 --> 00:07:03.990
conquer algorithms.

136
00:07:03.990 --> 00:07:05.770
You've seen a number of examples.

137
00:07:05.770 --> 00:07:08.890
And whenever we had to do a running
time analysis of such an algorithm we'd

138
00:07:08.890 --> 00:07:12.350
write out a recurrence we applied the
master method or in the worst case we'd

139
00:07:12.350 --> 00:07:15.880
run our recursion tree to figure out
the solution at our recurrence so

140
00:07:15.880 --> 00:07:20.080
you'd be very right to expect
something similar to happen here.

141
00:07:20.080 --> 00:07:22.650
But as we probe deeper and
we think about QuickSort

142
00:07:22.650 --> 00:07:25.470
we quickly realized that the master
method just doesn't apply, or

143
00:07:25.470 --> 00:07:29.650
at least not in the form that we're
used to, the problem is two fold.

144
00:07:29.650 --> 00:07:33.450
So first of all the size of the two
sub-problems is random, right?

145
00:07:33.450 --> 00:07:36.650
As we discuss in the last video,
the quality of the pivot

146
00:07:36.650 --> 00:07:40.610
is what determines how balanced the split
we get into the two sub-problems.

147
00:07:40.610 --> 00:07:44.690
It could be as bad as a sub-problem
of size 0 and one of size N minus 1.

148
00:07:44.690 --> 00:07:47.730
Or it could be as good as a perfectly
balanced split into two sub

149
00:07:47.730 --> 00:07:49.640
problems of equal sizes but we don't know.

150
00:07:49.640 --> 00:07:52.020
It's going to depend on
the random choice of the pivot.

151
00:07:52.020 --> 00:07:55.800
Moreover the master method at
least as we discussed it required

152
00:07:55.800 --> 00:07:58.220
solved subproblems to
have the same size and

153
00:07:58.220 --> 00:08:01.350
unless you're extremely lucky
that's not going to happen.

154
00:08:01.350 --> 00:08:02.460
In the QuickSort algorithm.

155
00:08:04.340 --> 00:08:07.450
It is possible to develop a theory
of recurrence relations for

156
00:08:07.450 --> 00:08:10.730
randomized algorithms and
apply that to QuickSort in particular.

157
00:08:10.730 --> 00:08:13.030
But I'm not going to go that route for
two reasons.

158
00:08:13.030 --> 00:08:14.580
The first one is't really quite messy.

159
00:08:14.580 --> 00:08:18.530
It get's pretty technical to talk
about solutions to recurrences for

160
00:08:18.530 --> 00:08:20.000
randomized algorithms.

161
00:08:20.000 --> 00:08:24.060
Or to thing about random recursion trees,
both of those get pretty complicated.

162
00:08:24.060 --> 00:08:25.086
The second reason is,

163
00:08:25.086 --> 00:08:28.336
I really want to introduce you to what
I call a decomposition principle.

164
00:08:28.336 --> 00:08:31.162
By which you take a random
variable that's complicated, but

165
00:08:31.162 --> 00:08:32.367
that you care about a lot.

166
00:08:32.367 --> 00:08:34.424
You decompose it into
simple random variables,

167
00:08:34.424 --> 00:08:37.948
which you don't really care about in their
own right, though it's easy analyze.

168
00:08:37.948 --> 00:08:41.280
And then you stitch those two things
together using linearity and expectation.

169
00:08:41.280 --> 00:08:45.248
So that's going to be the workhorse for
our analysis of the QuickSort algorithm.

170
00:08:45.248 --> 00:08:49.192
And it's going to come up again a couple
times in the rest of the course,

171
00:08:49.192 --> 00:08:51.379
for example, when we study hashing.

172
00:08:51.379 --> 00:08:54.319
So to explain how this decomposition
principle applies to QuickSort in

173
00:08:54.319 --> 00:08:54.907
particular.

174
00:08:54.907 --> 00:08:59.229
I'm going to need to introduce to you the
building blocks, simple random variables.

175
00:08:59.229 --> 00:09:02.781
Which will make up the complicated
random variable that we care about,

176
00:09:02.781 --> 00:09:04.300
the number of comparisons.

177
00:09:04.300 --> 00:09:06.280
Here's some notation.

178
00:09:06.280 --> 00:09:10.309
Recall that we fixed in the background
an arbitrary array of length n and

179
00:09:10.309 --> 00:09:12.065
that's denoted by capital A.

180
00:09:14.378 --> 00:09:17.870
And some notation which is simple but
also quite important.

181
00:09:17.870 --> 00:09:22.720
By z sub i,
what I mean is the ith smallest element

182
00:09:22.720 --> 00:09:26.970
in the input array capital A,
also know as the ith order statistic.

183
00:09:28.990 --> 00:09:31.070
So let me tell you what zi is not.

184
00:09:31.070 --> 00:09:33.220
What zi is not, in general,

185
00:09:33.220 --> 00:09:38.550
is the element in the ith position
of the input unsorted array.

186
00:09:38.550 --> 00:09:42.750
What zi is, is it's the element
which is going to wind up in the ith

187
00:09:42.750 --> 00:09:44.960
element of the array, once we sort it.

188
00:09:44.960 --> 00:09:48.415
Okay, so if you fast forward to the end
of a sorting algorithm and position i,

189
00:09:48.415 --> 00:09:49.493
you're going to find zi.

190
00:09:49.493 --> 00:09:51.792
So, let me give you an example.

191
00:09:51.792 --> 00:09:55.110
So suppose we had just
a simple array here,

192
00:09:55.110 --> 00:09:58.900
unsorted with the numbers 6, 8, 10 and 2.

193
00:09:58.900 --> 00:10:03.540
Then z1,

194
00:10:03.540 --> 00:10:07.730
well that's the first smallest,
the one smallest, or just the minimum.

195
00:10:07.730 --> 00:10:12.881
So z1 would be the 2, z2 would be the 6,
z3 would the the 8 and

196
00:10:12.881 --> 00:10:17.435
z4 would be the 10, for
this particular input array.

197
00:10:17.435 --> 00:10:20.260
Okay, so
zi is just the ith smallest number.

198
00:10:20.260 --> 00:10:24.570
Whatever it may lie on the original
unsorted array, that's what zi refers to.

199
00:10:26.110 --> 00:10:27.740
So we already defined the sample space.

200
00:10:27.740 --> 00:10:30.770
That's just all possible choices of
pivots the QuickSort might make.

201
00:10:30.770 --> 00:10:32.907
I already described one random variable,

202
00:10:32.907 --> 00:10:37.017
the number of comparisons that QuickSort
makes on a particular choice of pivots.

203
00:10:37.017 --> 00:10:41.278
Now I'm going to introduce a family
of much simpler random variables.

204
00:10:41.278 --> 00:10:46.286
Which count merely the comparisons
involving a given pair of

205
00:10:46.286 --> 00:10:51.897
elements in the input array,
not all elements, just a given pair.

206
00:10:51.897 --> 00:10:56.691
So for a given a choice of pivots,
a given sigma, and for

207
00:10:56.691 --> 00:11:01.601
given choices of inj,
both of which are between 1 and n.

208
00:11:01.601 --> 00:11:03.585
And so we only count things once, so

209
00:11:03.585 --> 00:11:06.107
I'm going to insist the i
is less than j always.

210
00:11:06.107 --> 00:11:10.360
And now here's a definition, my xij and
this is a random variable, so

211
00:11:10.360 --> 00:11:13.310
it's a function of the pivots chosen.

212
00:11:13.310 --> 00:11:17.400
This is going to be the number
of times that zi and

213
00:11:17.400 --> 00:11:20.794
zj are compared in
the execution of QuickSort.

214
00:11:22.260 --> 00:11:24.860
Okay, so this is going to be
an important definition in our analysis.

215
00:11:24.860 --> 00:11:26.580
It's important you understand it.

216
00:11:26.580 --> 00:11:30.939
So, for something like the third smallest
element and the seventh smallest element.

217
00:11:30.939 --> 00:11:35.826
xij is asking, that's when i equals 3 and
j equals 7, x37 is asking

218
00:11:35.826 --> 00:11:40.726
how many times those two elements
get compared as QuickSort proceeds.

219
00:11:40.726 --> 00:11:43.567
And this is a random variable in
the sense that if the pivot choices

220
00:11:43.567 --> 00:11:46.626
are all predetermined, if we think
of those being chosen in advance.

221
00:11:46.626 --> 00:11:50.226
Then there's just some fixed
deterministic number of times that zi and

222
00:11:50.226 --> 00:11:51.530
zj get compared.

223
00:11:51.530 --> 00:11:55.419
So it's important you understand
these random variables xij, so

224
00:11:55.419 --> 00:11:59.307
the next quiz is going to ask a basic
question about the range of values

225
00:11:59.307 --> 00:12:00.988
that a given xij can take on.

226
00:12:00.988 --> 00:12:04.569
So for this quiz we're considering
as usual some fixed input array.

227
00:12:04.569 --> 00:12:08.340
And now furthermore fixed to specific
elements of the input array.

228
00:12:08.340 --> 00:12:10.590
For example, the third smallest element,

229
00:12:10.590 --> 00:12:14.400
wherever it may lie, and the seventh
smallest element, wherever it may lie.

230
00:12:14.400 --> 00:12:17.180
Think about just these
pair of two elements.

231
00:12:17.180 --> 00:12:20.876
What is the range of values that
the corresponding random variable

232
00:12:20.876 --> 00:12:21.802
xij can take on?

233
00:12:21.802 --> 00:12:27.140
That is what are the different number of
times that a given pair of elements might

234
00:12:27.140 --> 00:12:32.174
be conceivably get compared in
the execution of the QuickSort algorithm?

235
00:12:32.174 --> 00:12:36.280
All right, so the correct answer
to this quiz is the second option.

236
00:12:36.280 --> 00:12:37.720
This is not a trivial quiz.

237
00:12:37.720 --> 00:12:39.270
This is a little tricky to see.

238
00:12:39.270 --> 00:12:41.473
So the assertion is that
a given pair of elements,

239
00:12:41.473 --> 00:12:43.000
they might not be compared at all.

240
00:12:43.000 --> 00:12:47.258
They might be compared once and they're
not going to get compared more than once.

241
00:12:47.258 --> 00:12:50.608
So here what I'm going to discuss
is why it's not possible for

242
00:12:50.608 --> 00:12:55.200
a given pair of elements to be compared
twice during the execution of QuickSort.

243
00:12:55.200 --> 00:12:58.620
It'll be clear later on, if it's not
already clear now, that both 0 and

244
00:12:58.620 --> 00:13:00.390
1 are legitimate possibilities.

245
00:13:00.390 --> 00:13:04.740
A pair of elements might never get
compared and they might get compared once.

246
00:13:04.740 --> 00:13:08.330
And again, we'll go into more
detail on that in the next video.

247
00:13:08.330 --> 00:13:10.690
But why is it impossible
to be compared twice?

248
00:13:10.690 --> 00:13:13.370
Well think about two elements, say
the third element and the seventh element.

249
00:13:13.370 --> 00:13:16.590
And let's recall how
the partition subroutine works.

250
00:13:16.590 --> 00:13:19.410
Observe that in QuickSort,
the only place in the code

251
00:13:19.410 --> 00:13:22.840
where comparisons between pairs
of input array elements happens.

252
00:13:22.840 --> 00:13:25.160
It only happens in
the partition subroutine, so

253
00:13:25.160 --> 00:13:26.460
that's where we have to drill down.

254
00:13:26.460 --> 00:13:30.320
So what are the comparisons that get
made in the partition subroutine?

255
00:13:30.320 --> 00:13:32.918
Well, go back and look at that code.

256
00:13:32.918 --> 00:13:37.712
The pivot element is compared
to each other element in

257
00:13:37.712 --> 00:13:40.488
the input array exactly once.

258
00:13:40.488 --> 00:13:42.830
So the pivot just hangs up in
the first entry of the array.

259
00:13:42.830 --> 00:13:46.348
We have this for loop, this index j which
marches over the rest of the array.

260
00:13:46.348 --> 00:13:47.817
And for each value of j,

261
00:13:47.817 --> 00:13:51.648
the jth element of the input
array gets compared to the pivot.

262
00:13:51.648 --> 00:13:55.554
So summarizing,
in an invocation of partition,

263
00:13:55.554 --> 00:13:59.838
every single comparison
involves the pivot element.

264
00:13:59.838 --> 00:14:03.168
So two elements get compared if and
only if one is the pivot.

265
00:14:03.168 --> 00:14:04.770
All right so
let's go back to the question.

266
00:14:04.770 --> 00:14:07.593
Why can't a given pair of elements of
the input array get compared two or

267
00:14:07.593 --> 00:14:08.630
more times?

268
00:14:08.630 --> 00:14:11.110
Well, think about the first time
they ever get compared in QuickSort.

269
00:14:12.280 --> 00:14:16.520
It must be the case, that at that
moment we're in a recursive call

270
00:14:16.520 --> 00:14:19.210
where either one of those
two is the pivot element.

271
00:14:19.210 --> 00:14:21.980
So if it's the third smallest element or
the seventh smallest element.

272
00:14:21.980 --> 00:14:24.460
The first time those two elements
are compared to each other,

273
00:14:24.460 --> 00:14:28.070
either the third smallest or the seventh
smallest is currently the pivot.

274
00:14:28.070 --> 00:14:31.120
Because all comparisons
involve a pivot element.

275
00:14:31.120 --> 00:14:33.760
Therefore, what's going to
happen in the recursion,

276
00:14:33.760 --> 00:14:37.350
well the pivot is excluded
from both recursive calls.

277
00:14:37.350 --> 00:14:41.020
So, for example, if the seventh smallest
element is currently the pivot, that's not

278
00:14:41.020 --> 00:14:45.310
going to be passed on the recursive call
which contains the third smallest element.

279
00:14:45.310 --> 00:14:48.600
Therefore if you're compared once,
one of the elements is the pivot and

280
00:14:48.600 --> 00:14:49.705
they'll never be compared again,

281
00:14:49.705 --> 00:14:54.160
because the pivot will not even show
up in any future recursive calls.

282
00:14:54.160 --> 00:14:56.689
So let me just remind
you of some terminology.

283
00:14:56.689 --> 00:14:59.281
So a random variable which can
only take on the values 0 or

284
00:14:59.281 --> 00:15:01.549
1 is often called
an indicator random variable,

285
00:15:01.549 --> 00:15:05.670
because it's just indicating whether or
not a certain things happens.

286
00:15:05.670 --> 00:15:09.060
So, in that terminology, each xij

287
00:15:11.820 --> 00:15:15.200
is indicating whether or not the ith
smallest element in the array and

288
00:15:15.200 --> 00:15:18.010
the jth smallest element in
the array ever get compared.

289
00:15:18.010 --> 00:15:20.330
It can't happen more than once,
it may or may not happen, and

290
00:15:20.330 --> 00:15:22.460
xij is 1 precisely when it happens.

291
00:15:22.460 --> 00:15:25.634
So that's the event that it's indicating.

292
00:15:25.634 --> 00:15:29.653
Having defined the building blocks I need,
these indicator random variables,

293
00:15:29.653 --> 00:15:30.428
these xij's.

294
00:15:30.428 --> 00:15:33.737
Now I can introduce you to
the decomposition principle as applied to

295
00:15:33.737 --> 00:15:34.381
QuickSort.

296
00:15:34.381 --> 00:15:37.285
So there's a random variable
that we really care about,

297
00:15:37.285 --> 00:15:41.760
which is denoted capital C, the number
of comparisons the QuickSort makes.

298
00:15:41.760 --> 00:15:45.370
That's really hard to get a handle on,
in and of itself, but

299
00:15:45.370 --> 00:15:50.300
we can express C as the sum of indicator
random variables, of these xijs.

300
00:15:50.300 --> 00:15:52.213
And those we don't care about
in their own right, but

301
00:15:52.213 --> 00:15:53.880
they're going to be much
easier to understand.

302
00:15:55.530 --> 00:15:58.930
So let me just rewrite the definitions of
C in the xij, so we're all clear on them.

303
00:16:01.850 --> 00:16:05.346
So c, recall, counts all of
the comparisons between pairs of

304
00:16:05.346 --> 00:16:09.797
input elements that QuickSort makes,
whereas an xij only counts the number.

305
00:16:09.797 --> 00:16:13.128
And it's going to be 0 or 1, comparisons
that involve the ith smallest and

306
00:16:13.128 --> 00:16:15.935
the jth smallest elements in particular.

307
00:16:15.935 --> 00:16:21.685
Now, since every comparison involves
precisely one pair of elements, some i and

308
00:16:21.685 --> 00:16:28.595
some j with i less than j,
we can write c as the sum of the xijs.

309
00:16:28.595 --> 00:16:30.915
So don't get intimidated
by this fancy double sum.

310
00:16:30.915 --> 00:16:34.429
All this is doing is it's iterating
over all of the ordered pairs, so

311
00:16:34.429 --> 00:16:37.457
all of the pairs ij, where i and
j are both between 1 and n and

312
00:16:37.457 --> 00:16:39.110
where i is strictly less than n.

313
00:16:39.110 --> 00:16:42.590
This double sum is just a convenient
way to do that iteration.

314
00:16:42.590 --> 00:16:46.081
And of course, no matter what the pivots
chosen are, we have this equality, okay?

315
00:16:46.081 --> 00:16:50.453
The comparisons are somehow split up
amongst the various pairs of elements,

316
00:16:50.453 --> 00:16:51.750
the various is and js.

317
00:16:51.750 --> 00:16:55.770
Why is it useful to express a complicated
random variable as a sum of simple random

318
00:16:55.770 --> 00:16:56.620
variables?

319
00:16:56.620 --> 00:16:59.850
Well, because an equation like this
is now right in the wheelhouse

320
00:16:59.850 --> 00:17:03.530
of linearity of expectation, so
let's just go ahead and apply that.

321
00:17:03.530 --> 00:17:07.450
Remember, and this is super, super
important, linearity of expectation says

322
00:17:07.450 --> 00:17:13.350
that the expectation of a sum
equals the sum of the expectations.

323
00:17:13.350 --> 00:17:15.820
And moreover, this is true whether or

324
00:17:15.820 --> 00:17:18.920
not the random variables are independent,
okay?

325
00:17:18.920 --> 00:17:21.530
And I'm not going to prove it here, but
you might want to think about the fact

326
00:17:21.530 --> 00:17:24.610
that the xijs are not,
in fact, independent.

327
00:17:24.610 --> 00:17:27.568
So we're using the fact that
linear expectation works even for

328
00:17:27.568 --> 00:17:31.120
non-independent random variables.

329
00:17:31.120 --> 00:17:32.670
Again, why is this interesting?

330
00:17:32.670 --> 00:17:38.750
Well, the left hand side,
This is complicated, right?

331
00:17:38.750 --> 00:17:43.716
This is some crazy number of comparisons
by some algorithm on some arbitrarily

332
00:17:43.716 --> 00:17:44.551
long array.

333
00:17:44.551 --> 00:17:49.140
And it fluctuates between two pretty far
apart numbers n log n and n squared.

334
00:17:49.140 --> 00:17:52.480
On the other hand,
this does not seem as intimidating.

335
00:17:52.480 --> 00:17:56.650
Given xij, it's just 0 or 1, whether or
not these two guys get compared or not.

336
00:17:58.630 --> 00:18:01.540
So that is the power of this
decomposition approach, okay?

337
00:18:01.540 --> 00:18:05.040
So, it reduces understanding
a complicated random variable

338
00:18:05.040 --> 00:18:07.880
to understanding simple random variables.

339
00:18:07.880 --> 00:18:10.250
In fact, because these
are indicator random variables,

340
00:18:10.250 --> 00:18:12.179
we can even clean up this
expression some more.

341
00:18:13.850 --> 00:18:19.190
So for any given xij being a 0, 1 random
variable, if we expand the definition

342
00:18:19.190 --> 00:18:24.150
of expectation, just as an average
over the various values, what is it?

343
00:18:24.150 --> 00:18:28.339
Well, it's some probability it takes
on the value 0, that's possible, and

344
00:18:28.339 --> 00:18:30.849
then some possibility it
takes on the value 1.

345
00:18:32.830 --> 00:18:38.710
And of course, this 0 part,
we can very satisfyingly delete, cancel.

346
00:18:38.710 --> 00:18:43.470
And so, the expected value of a given xij

347
00:18:43.470 --> 00:18:48.100
is just the probability that xij = 1.

348
00:18:48.100 --> 00:18:49.740
And remember,
it's an indicator random variable.

349
00:18:49.740 --> 00:18:55.180
It's 1 precisely when the ith smallest and
the jth smallest elements get compared.

350
00:18:55.180 --> 00:18:59.480
So putting it all together,
we find that what we care about.

351
00:18:59.480 --> 00:19:04.030
The average value of the number of
comparisons made by QuickSort on this

352
00:19:04.030 --> 00:19:09.179
input array is this double sum,
which literates over all ordered pairs,

353
00:19:10.960 --> 00:19:16.090
where each sum and is the probability
that the corresponding xij = 1.

354
00:19:16.090 --> 00:19:21.208
That is the probability that zi and

355
00:19:21.208 --> 00:19:23.860
zj get compared.

356
00:19:23.860 --> 00:19:27.617
And this is essentially the stopping
point for this video for the first

357
00:19:27.617 --> 00:19:31.829
part of the analysis, so let's call this
star and put a nice circle around it.

358
00:19:33.590 --> 00:19:37.940
So what's going to happen next is that
in the second video for the analysis,

359
00:19:37.940 --> 00:19:40.298
we're going to drill
down on this probability,

360
00:19:40.298 --> 00:19:44.090
probability that a given pair of elements
gets compared, and we're going to nail it.

361
00:19:44.090 --> 00:19:47.700
We're going to give an exact
expression as a function of i and j for

362
00:19:47.700 --> 00:19:49.650
exactly what this probability is.

363
00:19:49.650 --> 00:19:52.740
Then in the third video,
we're going to take that exact expression,

364
00:19:52.740 --> 00:19:55.890
plug it into the sum, and
then evaluate this sum.

365
00:19:55.890 --> 00:19:58.600
And it turns out the sum will
evaluate to O of n log n.

366
00:19:59.990 --> 00:20:00.740
So that's the plan.

367
00:20:00.740 --> 00:20:03.292
That's how you'll apply
decomposition in terms of 0, 1 or

368
00:20:03.292 --> 00:20:06.400
indicator random variables,
apply linearity of expectation.

369
00:20:06.400 --> 00:20:08.930
In the next video, we'll understand
these simple random variables, and

370
00:20:08.930 --> 00:20:10.980
then we'll wrap up in the third video.

371
00:20:10.980 --> 00:20:14.930
Before we move on to the next part of
the analysis, I do just want to emphasize

372
00:20:14.930 --> 00:20:17.920
that this decomposition principle is
relevant not only for QuickSort, but

373
00:20:17.920 --> 00:20:20.990
it's relevant for the analysis of
lots of randomized algorithms.

374
00:20:20.990 --> 00:20:22.300
And we will see more applications,

375
00:20:22.300 --> 00:20:24.420
at least one more application,
later in the course.

376
00:20:24.420 --> 00:20:27.540
So just to kind of really
hammer the point home,

377
00:20:27.540 --> 00:20:31.000
let me spell out the key steps for
the general decomposition principle.

378
00:20:32.240 --> 00:20:34.160
So first you need to figure
out what is it you care about.

379
00:20:34.160 --> 00:20:36.780
So in QuickSort,
we cared about the number of comparisons.

380
00:20:36.780 --> 00:20:39.850
We had this lemma that said the running
time is dominated by comparisons.

381
00:20:39.850 --> 00:20:43.609
So we understood what we wanted to know,
the average value for

382
00:20:43.609 --> 00:20:45.357
the number of comparisons.

383
00:20:45.357 --> 00:20:49.814
The second step is to express this random
variable y as a sum of simple random

384
00:20:49.814 --> 00:20:53.520
variables, ideally indicator or
0, 1 random variables.

385
00:20:56.160 --> 00:21:01.617
Now you're in the wheel house
of linearity of expectation,

386
00:21:01.617 --> 00:21:07.181
you just apply it, and
you find that what it is you care about,

387
00:21:07.181 --> 00:21:11.568
the average value of
the random variable y is just

388
00:21:11.568 --> 00:21:15.970
the sum of the probabilities
of various events.

389
00:21:15.970 --> 00:21:21.346
That given xl,
random variable is equal to 1.

390
00:21:21.346 --> 00:21:26.174
And so the upshot is to understand the
seemingly very complicated left-hand side,

391
00:21:26.174 --> 00:21:29.914
all you have to do is understand
something, which in many cases,

392
00:21:29.914 --> 00:21:34.550
is much simpler, which is understand
the probability of these various events.

393
00:21:34.550 --> 00:21:38.470
In the next video, I'll show you exactly
how that's done in the case of QuickSort,

394
00:21:38.470 --> 00:21:39.790
where we care about the xijs,

395
00:21:39.790 --> 00:21:42.510
the probability that two
elements gets compared.

396
00:21:42.510 --> 00:21:45.820
So let's move on and
get exact expression for that probability.