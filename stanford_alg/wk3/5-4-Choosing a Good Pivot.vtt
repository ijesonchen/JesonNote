WEBVTT

1
00:00:01.160 --> 00:00:02.790
So let's review the story so far.

2
00:00:02.790 --> 00:00:04.490
We've been discussing
the QuickSort algorithm.

3
00:00:04.490 --> 00:00:06.600
Here again is its high level description.

4
00:00:06.600 --> 00:00:09.690
So in QuickSort you call
two subroutines first, and

5
00:00:09.690 --> 00:00:11.940
then you make two recursive calls.

6
00:00:11.940 --> 00:00:15.005
So the first subroutine ChoosePivot,
we haven't discussed yet at all.

7
00:00:15.005 --> 00:00:17.870
That'll be one of the main
topics of this video.

8
00:00:17.870 --> 00:00:21.330
But the job of the ChoosePivot
subroutine is to somehow select

9
00:00:21.330 --> 00:00:24.890
one of the n elements in the input array,
to act as a pivot element.

10
00:00:24.890 --> 00:00:26.460
Now what does it mean to be a pivot?

11
00:00:26.460 --> 00:00:28.503
Well that comes into play
in the second subroutine,

12
00:00:28.503 --> 00:00:32.370
the partition subroutine, which we did
discuss quite a bit in a previous video.

13
00:00:32.370 --> 00:00:35.710
So what a partition does is it rearranges
the elements in the input array, so that

14
00:00:35.710 --> 00:00:40.170
it has the following property, so that the
pivot p winds up in its rightful position.

15
00:00:40.170 --> 00:00:43.390
That is, it's to the right of all
of the elements less than it, and

16
00:00:43.390 --> 00:00:45.550
it's to the left of all of
the elements bigger than it.

17
00:00:45.550 --> 00:00:48.620
The stuff less than it's to
the left in some jumbled order.

18
00:00:48.620 --> 00:00:51.630
The stuff bigger than it's to
the right in some jumbled order.

19
00:00:51.630 --> 00:00:53.560
That's what's listed here
as the first part and

20
00:00:53.560 --> 00:00:56.010
the second part of the partitioned array.

21
00:00:56.010 --> 00:00:58.440
Now, once you've done this partitioning,
you're good to go.

22
00:00:58.440 --> 00:01:03.760
You just recursively sort the first
part to get them in the right order,

23
00:01:03.760 --> 00:01:06.790
you call QuickSort again to
recursively sort the right part, and

24
00:01:06.790 --> 00:01:08.380
bingo, the entire array is sorted.

25
00:01:08.380 --> 00:01:11.102
You don't need a combine step,
you don't need a merge step.

26
00:01:11.102 --> 00:01:12.730
Where we'll recall in a previous video,

27
00:01:12.730 --> 00:01:15.770
we saw that the partition array
can be implemented in linear time.

28
00:01:15.770 --> 00:01:19.590
And moreover, it works in place with
essentially no additional storage.

29
00:01:19.590 --> 00:01:23.260
We also, in an optional video, formally
proved the correctness of QuickSort, and

30
00:01:23.260 --> 00:01:27.770
remember QuickSort is independent of how
you implement the ChoosePivot subroutine.

31
00:01:27.770 --> 00:01:31.211
So what we're going to do now is discuss
the running time of the QuickSort

32
00:01:31.211 --> 00:01:34.785
algorithm, and this is where the choice
of the pivot is very important.

33
00:01:37.177 --> 00:01:39.921
So what everybody should be
wondering about at this point is,

34
00:01:39.921 --> 00:01:41.358
is QuickSort a good algorithm?

35
00:01:41.358 --> 00:01:42.300
Does it run fast?

36
00:01:42.300 --> 00:01:43.280
The bar's pretty high.

37
00:01:43.280 --> 00:01:47.090
We already have MergeSort, which is a very
excellent, practical n log n algorithm.

38
00:01:52.233 --> 00:01:57.039
The key point to realize at this juncture,
is that we are not currently in a position

39
00:01:57.039 --> 00:02:00.360
to discuss the running time
of the QuickSort algorithm.

40
00:02:00.360 --> 00:02:02.620
The reason is we do not
have enough information.

41
00:02:02.620 --> 00:02:06.870
The running time of QuickSort depends
crucially on how you choose the pivot.

42
00:02:06.870 --> 00:02:10.034
It depends crucially on
the quality of the pivot chosen.

43
00:02:15.828 --> 00:02:18.740
You'd be right to wonder what
I mean by a pivot's quality.

44
00:02:18.740 --> 00:02:21.310
And basically what I mean,
is a pivot is good

45
00:02:21.310 --> 00:02:25.370
if it splits the partitioned array into
roughly two equal sized subproblems.

46
00:02:25.370 --> 00:02:29.190
And it's bad, it's of low quality,
if we get very unbalanced subproblems.

47
00:02:29.190 --> 00:02:32.788
So to understand both, what I mean,
and the ramifications of having good

48
00:02:32.788 --> 00:02:36.804
quality and bad quality pivots, let's
walk through a couple of quiz questions.

49
00:02:39.637 --> 00:02:43.733
This first quiz question is meant to
explore a sort of worst case execution of

50
00:02:43.733 --> 00:02:45.220
the QuickSort algorithm.

51
00:02:45.220 --> 00:02:48.900
What happens when you choose pivots
that are very poorly suited for

52
00:02:48.900 --> 00:02:50.730
the particular input array?

53
00:02:50.730 --> 00:02:51.890
Let me be more specific.

54
00:02:51.890 --> 00:02:55.170
Suppose we use the most naive
ChoosePivot implementation,

55
00:02:55.170 --> 00:02:58.220
like we were discussing
in the partition video.

56
00:02:58.220 --> 00:03:01.760
So remember, here we just pluck out
the first element of the array and

57
00:03:01.760 --> 00:03:03.460
we use that as the pivot.

58
00:03:03.460 --> 00:03:06.470
So suppose that's how we implement
the ChoosePivot subroutine, and

59
00:03:06.470 --> 00:03:10.800
moreover, suppose that
the input array to QuickSort

60
00:03:10.800 --> 00:03:13.290
is an array that's
already in sorted order.

61
00:03:13.290 --> 00:03:16.360
So for example, if it just had the numbers
one through eight, it would be one, two,

62
00:03:16.360 --> 00:03:18.820
three, four, five, six,
seven, eight, in order.

63
00:03:18.820 --> 00:03:22.570
My question for you is,
what is the running time

64
00:03:22.570 --> 00:03:26.970
of this recursive QuickSort algorithm
on an already sorted array,

65
00:03:26.970 --> 00:03:31.510
if we always use the first element
of a subarray as the pivot?

66
00:03:34.280 --> 00:03:37.160
Okay, so this is a slightly tricky, but
actually a very important question.

67
00:03:37.160 --> 00:03:39.400
So the answer is the fourth one.

68
00:03:39.400 --> 00:03:43.250
So it turns out, that QuickSort,
if you pass it an already sorted array and

69
00:03:43.250 --> 00:03:47.620
you're using the first element as pivot
elements, it runs in quadratic time.

70
00:03:47.620 --> 00:03:51.460
And remember for a sorting algorithm,
quadratic is bad.

71
00:03:51.460 --> 00:03:53.525
It's bad in the sense
that we can do better.

72
00:03:53.525 --> 00:03:57.240
MergeSort runs in time n log n,
which is much better than n squared.

73
00:03:57.240 --> 00:03:58.900
And if we we're happy with
an n squared running time,

74
00:03:58.900 --> 00:04:03.240
we wouldn't have to resort to these sort
of relatively exotic sorting algorithms.

75
00:04:03.240 --> 00:04:05.240
We could just use Insertion sort,
and we'd be fine.

76
00:04:05.240 --> 00:04:07.560
We'd get that same quadratic running time.

77
00:04:07.560 --> 00:04:09.130
Okay, so now I owe you an explanation.

78
00:04:09.130 --> 00:04:13.544
Why is it that QuickSort can actually run
in quadratic time, in this unlucky case,

79
00:04:13.544 --> 00:04:16.079
of being passed an already
sorted input array?

80
00:04:20.001 --> 00:04:23.164
Well to understand, let's think
about what pivot gets chosen, and

81
00:04:23.164 --> 00:04:27.035
what are the ramifications of that pivot
choice for how the array gets partitioned,

82
00:04:27.035 --> 00:04:29.300
and then what the recursion looks like.

83
00:04:29.300 --> 00:04:33.730
So, let's just think of the array as being
the numbers 1 through n, in sorted order.

84
00:04:33.730 --> 00:04:34.970
What is going to be our pivot?

85
00:04:34.970 --> 00:04:37.440
Well, by definition we're choosing
the first element of the pivot, so

86
00:04:37.440 --> 00:04:39.410
the pivot's just going to be 1.

87
00:04:39.410 --> 00:04:41.850
Now we're going to invoke
the partition subroutine.

88
00:04:41.850 --> 00:04:44.710
And if you go back to the Pseudocode of
the partition subroutine, you'll notice

89
00:04:44.710 --> 00:04:48.421
that if we pass an already sorted array,
it's going to do essentially nothing.

90
00:04:48.421 --> 00:04:50.750
Okay?
So it's just going to advance the index j,

91
00:04:50.750 --> 00:04:54.280
until it falls off the end of the array,
and it's just going to return back to us,

92
00:04:54.280 --> 00:04:56.420
the same array that it
was passed as input.

93
00:04:56.420 --> 00:04:58.110
So partition subroutine,

94
00:04:58.110 --> 00:05:02.380
if given an already sorted array,
returns an already sorted array.

95
00:05:02.380 --> 00:05:04.770
So we have just a pivot 1,
in the first position.

96
00:05:04.770 --> 00:05:09.126
And then the numbers 2 through n, in
order, in the remainder of the positions.

97
00:05:11.541 --> 00:05:15.512
So if we draw our usual picture of
what a partitioned array looks like,

98
00:05:15.512 --> 00:05:18.271
with everything less than
the pivot to the left,

99
00:05:18.271 --> 00:05:21.530
everything bigger than
the pivot to the right.

100
00:05:21.530 --> 00:05:26.520
Well, since nothing is less than the
pivot, this stuff is going to be empty.

101
00:05:26.520 --> 00:05:28.700
This will not exist.

102
00:05:28.700 --> 00:05:32.580
And to the right of the pivot,
this will have length n- 1, and

103
00:05:32.580 --> 00:05:35.440
moreover, it will still be sorted.

104
00:05:37.610 --> 00:05:38.930
So once partition completes,

105
00:05:38.930 --> 00:05:42.830
we go back to the outer call of QuickSort,
which then calls itself recursively twice.

106
00:05:42.830 --> 00:05:45.220
Now in this case, one of
the recursive calls is just vacuous.

107
00:05:45.220 --> 00:05:47.190
There's just an empty array,
there's nothing to do.

108
00:05:47.190 --> 00:05:49.340
So really there's only one recursive call,

109
00:05:49.340 --> 00:05:53.390
and that happens on a problem
of size only one less.

110
00:05:53.390 --> 00:05:56.840
So this is about the most unbalanced
split we could possibly see, right,

111
00:05:56.840 --> 00:05:59.940
where one side has 0 elements,
one side's n- 1.

112
00:05:59.940 --> 00:06:02.000
Splits don't really get
any worse than that.

113
00:06:02.000 --> 00:06:06.540
And this is going to keep happening over,
and over, and over again.

114
00:06:06.540 --> 00:06:08.520
We're going to recurse on
the numbers 2 through n.

115
00:06:08.520 --> 00:06:12.010
We're going to choose the first element,
the 2, as the pivot.

116
00:06:12.010 --> 00:06:13.400
Again, we'll feed it to partition.

117
00:06:13.400 --> 00:06:15.910
We'll get back the exact same
subarray that we handed it in.

118
00:06:15.910 --> 00:06:18.410
We get to the numbers 2 through n,
in sorted order.

119
00:06:18.410 --> 00:06:19.500
We exclude the pivot 2,

120
00:06:19.500 --> 00:06:24.210
we recurse on the numbers 3 through n,
a subarray of length n- 2.

121
00:06:24.210 --> 00:06:28.500
The next recursion level, we recurse
on an array of size of length n- 3,

122
00:06:28.500 --> 00:06:31.250
then n- 4, then n- 5, and so on.

123
00:06:31.250 --> 00:06:34.870
Until finally,
after I did recursion depth of n,

124
00:06:34.870 --> 00:06:39.340
roughly, we got down to just the last
element n, the base case kicks in, and

125
00:06:39.340 --> 00:06:42.230
we return that, and QuickSort completes.

126
00:06:42.230 --> 00:06:45.140
So that's how QuickSort is going to
execute on this particular input with

127
00:06:45.140 --> 00:06:49.150
these particular pivot choices, so
what running time does that give to us?

128
00:06:50.200 --> 00:06:53.800
Well, the first observation is
that in each recursive call,

129
00:06:53.800 --> 00:06:56.780
we do have to invoke
the partition subroutine.

130
00:06:56.780 --> 00:07:01.253
And the partition subroutine does look at
every element in the array it has passed

131
00:07:01.253 --> 00:07:01.970
as input.

132
00:07:01.970 --> 00:07:06.216
So if we pass partition in array of length
k, it's going to do at least k operations,

133
00:07:06.216 --> 00:07:08.440
because it looks at each
element at least once.

134
00:07:08.440 --> 00:07:13.630
So the runtime is going to be bounded
below by the work we do in the outermost

135
00:07:13.630 --> 00:07:18.690
call, which is on an array of length n,
plus the amount we do in the second

136
00:07:18.690 --> 00:07:24.340
level of recursion, which is on a subarray
of length (n- 1) + (n- 2) +, blah, blah,

137
00:07:24.340 --> 00:07:30.060
blah, blah, blah, all the way down to + 1,
for the very last level of the recursion.

138
00:07:30.060 --> 00:07:31.860
So this is a lower bound
on our running time,

139
00:07:31.860 --> 00:07:35.050
and this is already Theta of n squared.

140
00:07:36.920 --> 00:07:40.791
So, one easy way to see why this
sum n + (n- 1) +, etc., etc.,

141
00:07:40.791 --> 00:07:45.036
leads to a bound of n squared, is to just
focus on the first half of the terms.

142
00:07:45.036 --> 00:07:49.961
So, the first n over two terms in the sum
are all of magnitude at least n over 2, so

143
00:07:49.961 --> 00:07:52.219
the sum is at least n squared over 4.

144
00:07:52.219 --> 00:07:55.595
It's also evident that this
sum is at most, n squared.

145
00:07:55.595 --> 00:07:56.713
So, overall,

146
00:07:56.713 --> 00:08:01.969
the running time of QuickSort on this
bad input is going to be quadratic.

147
00:08:01.969 --> 00:08:04.449
Now having understood what
the worst case performance for

148
00:08:04.449 --> 00:08:08.440
the QuickSort algorithm is, lets move on
to discuss it's best case running time.

149
00:08:08.440 --> 00:08:11.590
Now we don't generally care about the best
case performance of algorithms for

150
00:08:11.590 --> 00:08:12.590
it's own sake.

151
00:08:12.590 --> 00:08:15.680
The reason that we want to think
about QuickSort in the best case,

152
00:08:15.680 --> 00:08:18.970
first of all it'll give us better
intuition for how the algorithm works.

153
00:08:18.970 --> 00:08:21.235
Second of all,
it'll draw a line in the sand.

154
00:08:21.235 --> 00:08:24.850
Its average case running time certainly
can't be better than the best case, so

155
00:08:24.850 --> 00:08:27.140
this will give us a target for
what we're shooting for

156
00:08:27.140 --> 00:08:29.300
in our subsequent mathematical analysis.

157
00:08:31.660 --> 00:08:32.720
So what were the best case?

158
00:08:32.720 --> 00:08:35.230
What was the highest quality
pivot we could hope for?

159
00:08:35.230 --> 00:08:38.540
Well again, we think of the quality
of the pivot as the amount of

160
00:08:38.540 --> 00:08:40.880
balance that it provides
between the two sub problems.

161
00:08:40.880 --> 00:08:44.620
So ideally, we choose a pivot
which gave us two sub-problems,

162
00:08:44.620 --> 00:08:47.170
both of size n over 2 or less.

163
00:08:47.170 --> 00:08:48.860
And there's a name for

164
00:08:48.860 --> 00:08:52.400
the element that would give us
that perfectly balanced split.

165
00:08:52.400 --> 00:08:55.930
It's the median element of the array,
okay, the element where exactly half of

166
00:08:55.930 --> 00:08:59.380
the elements are less than it and
half of the elements are bigger than it.

167
00:08:59.380 --> 00:09:03.130
That would give us an essentially
perfect 50, 50 split of the input array.

168
00:09:03.130 --> 00:09:04.340
So, here's the question.

169
00:09:04.340 --> 00:09:07.210
Suppose we had some input and
we ranked QuickSort, and

170
00:09:07.210 --> 00:09:11.550
everything just worked in our favor, in
the magically, in the best possible way.

171
00:09:11.550 --> 00:09:15.040
That is, in every single
recursive invocation of QuickSort,

172
00:09:15.040 --> 00:09:17.380
on any sub array of
the original input array.

173
00:09:17.380 --> 00:09:22.150
Suppose, we happen to get,
as our pivot the median element.

174
00:09:22.150 --> 00:09:24.960
That is,
suppose in every single recursive call.

175
00:09:24.960 --> 00:09:29.980
We wind up getting a perfect 50/50 split
of the input array before we recurse.

176
00:09:29.980 --> 00:09:34.329
This question asks you to analyze
the running time of this algorithm in

177
00:09:34.329 --> 00:09:36.479
this magical best case scenario.

178
00:09:38.907 --> 00:09:42.309
So the answer to this
question is the third option.

179
00:09:42.309 --> 00:09:45.269
It runs (n log n) times.

180
00:09:47.591 --> 00:09:48.210
Why is that?

181
00:09:48.210 --> 00:09:51.240
Well, the reason is then the recurrence
which governs the running time of

182
00:09:51.240 --> 00:09:54.015
QuickSort exactly matches the recurrence

183
00:09:54.015 --> 00:09:56.750
short running time that we
already know is n log n.

184
00:09:59.225 --> 00:10:02.015
That is the running time
QuickSort requires in this

185
00:10:02.015 --> 00:10:04.855
magical special case on
a array of length n.

186
00:10:04.855 --> 00:10:07.345
As usual,
you have a recurrence in two parts.

187
00:10:07.345 --> 00:10:09.415
There's the work that gets done
by the recursive cause and

188
00:10:09.415 --> 00:10:11.665
there's the work that gets done now.

189
00:10:11.665 --> 00:10:15.090
Now by assumption, we wind up
picking the median as the pivot.

190
00:10:15.090 --> 00:10:17.570
So there's going to be
two recursive calls,

191
00:10:17.570 --> 00:10:21.960
each of which will be on an input
of size at most N over two.

192
00:10:21.960 --> 00:10:26.140
And, we can write this, this is
because the pivot equals the median.

193
00:10:27.470 --> 00:10:29.690
So this is not true for
quick sort of general,

194
00:10:29.690 --> 00:10:32.219
it's only true in this magical case,
where the pivot is the median.

195
00:10:34.250 --> 00:10:36.860
So that's what gets done
by the two recursive calls.

196
00:10:36.860 --> 00:10:39.740
And then how much work do we do
outside of the recursive calls?

197
00:10:39.740 --> 00:10:41.830
Well, we have to do
the truth pivot subroutine.

198
00:10:41.830 --> 00:10:45.120
And I guess, strictly speaking,
I haven't said how that was implemented.

199
00:10:45.120 --> 00:10:48.200
But let's assume that choose pivot
does only a linear amount of work.

200
00:10:48.200 --> 00:10:49.090
And then, as we've seen,

201
00:10:49.090 --> 00:10:52.840
the partition subroutine only does
a linear amount of work, as well.

202
00:10:52.840 --> 00:10:57.198
So let's say O(n), for
work outside of the recursive calls.

203
00:10:59.450 --> 00:11:00.553
And what do we know?

204
00:11:00.553 --> 00:11:04.157
We know this implies,
say by using the master method, or

205
00:11:04.157 --> 00:11:09.113
just by using the exact same argument as
for Merge Sort, this gives us a running

206
00:11:09.113 --> 00:11:14.753
time balunt of (nlogn) And again something
I haven't really been emphasizing but

207
00:11:14.753 --> 00:11:17.300
which is true is that actually
we can write theta (n log n).

208
00:11:17.300 --> 00:11:20.720
And that's because in the recurrence, in
fact, we know that the work done outside

209
00:11:20.720 --> 00:11:22.780
of the recursive calls is
exactly theta (n), okay?

210
00:11:22.780 --> 00:11:26.260
Partition needs really linear time,
not just O(n) time.

211
00:11:26.260 --> 00:11:28.770
In fact the work done outside of
the recursive calls is theta (n).

212
00:11:28.770 --> 00:11:31.650
That's because the partition
serpentine does indeed look at

213
00:11:31.650 --> 00:11:34.530
every entry in the array that it passed,
all right, and as a result,

214
00:11:34.530 --> 00:11:36.640
we didn't really discuss this so
much in the master method.

215
00:11:36.640 --> 00:11:41.240
But as I mentioned in passing, if you have
recurrences which are tight in this sense,

216
00:11:41.240 --> 00:11:43.990
then the result of the master method

217
00:11:43.990 --> 00:11:46.390
can also be strengthened to be
theta instead of just beta.

218
00:11:46.390 --> 00:11:50.230
But those are just some extra details.

219
00:11:50.230 --> 00:11:54.260
The upshot of this quiz is that even in
the best case, even if we magically get

220
00:11:54.260 --> 00:11:57.400
prefect pivots throughout the entire
trajectory of quick sort.

221
00:11:57.400 --> 00:11:59.610
The best we can hope for
is an n log n upper bound,

222
00:11:59.610 --> 00:12:01.410
it's not going to get
any better than that.

223
00:12:01.410 --> 00:12:06.610
So the question is how do we have a
principled way of choosing pivots so that

224
00:12:06.610 --> 00:12:11.080
we get this best case or something like
it that's best case n log n running time.

225
00:12:11.080 --> 00:12:13.390
So that's what the problem
that we have to solve next.

226
00:12:13.390 --> 00:12:19.100
So the last couple quizzes have identified
a super important question, as far as

227
00:12:19.100 --> 00:12:24.520
the implementation of Quicksort, which is
how are we going to choose these pivots?

228
00:12:24.520 --> 00:12:28.000
We now know that they have a big influence
on the running time of our algorithm.

229
00:12:28.000 --> 00:12:29.400
It could be as bad as n squared or

230
00:12:29.400 --> 00:12:32.305
as good as m log n, and
we really want to be on the m log n side.

231
00:12:32.305 --> 00:12:37.140
So the key question: how to choose pivots.

232
00:12:41.870 --> 00:12:46.001
And quick sort is the first killer
application that we're going to see

233
00:12:46.001 --> 00:12:50.694
of the idea of randomized algorithms,
that is allowing your algorithms to flip

234
00:12:50.694 --> 00:12:55.050
coins in the code so that you get some
kind of good performance on average.

235
00:12:58.256 --> 00:13:03.085
So the big idea is random pivots.

236
00:13:05.604 --> 00:13:09.860
By which I mean for every time we
recursively call quick sort and

237
00:13:09.860 --> 00:13:12.840
we are pass some subarray of length k.

238
00:13:12.840 --> 00:13:15.920
Among the K candidate pivot
elements in the sub-array,

239
00:13:15.920 --> 00:13:20.700
we're going to choose each one
with probability of one over k.

240
00:13:20.700 --> 00:13:23.460
And we're going to make a new random
choice every time we have qa recursive

241
00:13:23.460 --> 00:13:26.500
call, and
we're going to see how the algorithm does.

242
00:13:26.500 --> 00:13:28.750
This is our first example
of a randomized algorithm.

243
00:13:28.750 --> 00:13:32.590
This is an algorithm where,
if you feed it exactly the same input,

244
00:13:32.590 --> 00:13:35.130
it'll actually run differently,
on different execution.

245
00:13:35.130 --> 00:13:38.450
And that's because there's randomness
internal to the code of the algorithm.

246
00:13:38.450 --> 00:13:40.840
Now, it's not necessarily intuitive.

247
00:13:40.840 --> 00:13:45.580
The randomization should have any purpose
in the computation, in software design and

248
00:13:45.580 --> 00:13:46.730
algorithm design.

249
00:13:46.730 --> 00:13:49.890
But, in fact, and this has been sort of
one of the real breakthroughs in algorithm

250
00:13:49.890 --> 00:13:53.380
design, mostly in the '70s,
in realizing how important this is,

251
00:13:53.380 --> 00:13:57.400
that the use of randomization can
make algorithms more elegant.

252
00:13:57.400 --> 00:14:02.210
Simpler, easier to code, faster,
or just simply you can solve

253
00:14:02.210 --> 00:14:07.170
problems that you could not solve as
easily without the use of randomization.

254
00:14:07.170 --> 00:14:11.190
It's really one thing that should be in
your toolbox as an algorithm designer,

255
00:14:11.190 --> 00:14:12.280
randomization.

256
00:14:12.280 --> 00:14:14.605
Quick Sort will be the first
[INAUDIBLE] application, but

257
00:14:14.605 --> 00:14:16.453
we'll see a couple more
later in the course.

258
00:14:18.636 --> 00:14:22.504
Now by the end of this sequence of video's
I'll have given you a complete rigorous

259
00:14:22.504 --> 00:14:24.030
argument about why this works.

260
00:14:24.030 --> 00:14:28.480
Why with random pivots, quick sort
always runs very quickly, on average.

261
00:14:28.480 --> 00:14:32.360
But, you know, before moving into anything
to formal let's develop a little bit of

262
00:14:32.360 --> 00:14:34.690
intuition or at least kind of a daydream.

263
00:14:34.690 --> 00:14:38.240
About why on Earth could this possibly
work, why on Earth could this possibly be

264
00:14:38.240 --> 00:14:43.210
a good idea, to have randomness internal
to our pro sort implementation.

265
00:14:43.210 --> 00:14:48.120
Well, so first just very high level, what
would be sort of the hope, or the dream.

266
00:14:48.120 --> 00:14:50.800
The hope would be, random pivot's
are not going to be perfect,

267
00:14:50.800 --> 00:14:53.050
I mean you're not going to just
sort of guess the median, or

268
00:14:53.050 --> 00:14:55.960
you only have a one in chance of
figuring out which one the median is,

269
00:14:55.960 --> 00:14:59.730
but the hope is that most choices
of a pivot will be good enough.

270
00:15:02.040 --> 00:15:03.060
So that's pretty fuzzy.

271
00:15:03.060 --> 00:15:05.690
Let's drill down a little bit and
develop this intuition further.

272
00:15:06.730 --> 00:15:08.430
Let me describe it in two steps.

273
00:15:12.200 --> 00:15:15.840
The first claim is that, you know in our
last quiz we said suppose we get lucky and

274
00:15:15.840 --> 00:15:18.970
we always pick the median in
every single recursive call.

275
00:15:18.970 --> 00:15:20.050
And we observed we'd do great.

276
00:15:20.050 --> 00:15:21.310
We'd get end log in running time.

277
00:15:21.310 --> 00:15:24.560
So now let's observe that actually
to get the end log in running time,

278
00:15:24.560 --> 00:15:28.550
it's not important that we magically get
the median every single recursive call.

279
00:15:28.550 --> 00:15:32.590
If we get any kind of reasonable pivot, by
which a pivot that gives us some kind of

280
00:15:32.590 --> 00:15:36.570
approximately balanced with the problems,
again, we're going to be good.

281
00:15:36.570 --> 00:15:40.450
So the last quiz really wasn't
particular to getting the exact median.

282
00:15:40.450 --> 00:15:42.620
Near medians are also fine.

283
00:15:42.620 --> 00:15:47.422
To be concrete, suppose we always pick
a pivot which guarantees us a split

284
00:15:47.422 --> 00:15:49.285
of 25 to 75, or better.

285
00:15:49.285 --> 00:15:54.310
That is, both recursive calls
should be called on arrays of size,

286
00:15:54.310 --> 00:15:57.643
at most, 75% of the one we started with.

287
00:15:57.643 --> 00:16:02.767
So precisely if we always get a 25, 75
split or better in every recursive call I

288
00:16:02.767 --> 00:16:07.541
claim that the running time of quick sort
in that event will be big O of n log n.

289
00:16:07.541 --> 00:16:10.404
Just as it was in the last quiz where
we're actually assuming something much

290
00:16:10.404 --> 00:16:12.670
stronger where we're getting a median.

291
00:16:12.670 --> 00:16:16.954
Now this is not so obvious, the fact
that 25, 75 splits guarantee analog and

292
00:16:16.954 --> 00:16:17.778
running time.

293
00:16:17.778 --> 00:16:20.800
For those of you that are feeling keen
you might want to try to prove this.

294
00:16:20.800 --> 00:16:23.830
You can prove this using
a recursion tree argument, but

295
00:16:23.830 --> 00:16:26.430
because you don't have balanced sub
problems you have to work a little bit

296
00:16:26.430 --> 00:16:29.050
harder than you do in the cases
covered by the master method.

297
00:16:30.670 --> 00:16:32.654
So that's the first part of the intuition,
and

298
00:16:32.654 --> 00:16:34.696
this is what we mean by
a pivot being good enough.

299
00:16:34.696 --> 00:16:39.023
If we get a 25, 75 split or better,
we're good to go, we get our desired,

300
00:16:39.023 --> 00:16:40.150
our target analog.

301
00:16:41.470 --> 00:16:45.713
The second part of the intuition is to
realize that actually we don't have to

302
00:16:45.713 --> 00:16:48.917
get all that lucky to just
be getting a 25, 75 split.

303
00:16:48.917 --> 00:16:50.468
That's actually a pretty modest goal and

304
00:16:50.468 --> 00:16:52.958
even this modest goal is enough to
get n log n running time, right?

305
00:16:52.958 --> 00:16:56.986
So suppose our array contains the numbers,
the integers between 1 and

306
00:16:56.986 --> 00:16:59.484
100, so it is an array of length 100.

307
00:16:59.484 --> 00:17:04.300
Think for a second, which of those
elements is going to give us

308
00:17:04.300 --> 00:17:07.368
a split that's 25, 75 or better?

309
00:17:07.368 --> 00:17:11.210
So, if we pick any element between 26 and

310
00:17:11.210 --> 00:17:15.600
75 inclusive, will be totally good, right?

311
00:17:15.600 --> 00:17:17.920
If we pick something that's at least 26,

312
00:17:17.920 --> 00:17:22.220
that means the left subproblem is going to
have at least the elements 1 through 25.

313
00:17:22.220 --> 00:17:24.670
That'll have at least 25% of the elements.

314
00:17:24.670 --> 00:17:28.531
If we pick something less than 75 then
the right sub-problem will have all of

315
00:17:28.531 --> 00:17:31.339
the elements 76 through
100 after we partition, so

316
00:17:31.339 --> 00:17:33.927
that'll also have at least
25% of the elements.

317
00:17:33.927 --> 00:17:38.510
So anything between 26 and
75 gives us a 75-25 split or better.

318
00:17:39.610 --> 00:17:42.166
But that's a full half of the elements, so

319
00:17:42.166 --> 00:17:45.867
it's as good as just flipping
a fair coin hoping to get heads.

320
00:17:45.867 --> 00:17:47.893
So with 50% probability,

321
00:17:47.893 --> 00:17:51.870
we get a split that's good enough
to get this n log n bound.

322
00:17:51.870 --> 00:17:56.378
And so again, the high level hope is
that often enough, half of the time,

323
00:17:56.378 --> 00:18:00.380
we get these good enough splits,
25-75 split or better, so

324
00:18:00.380 --> 00:18:05.220
that would seem to suggest n log n running
time on average is a legitimate hope.

325
00:18:07.660 --> 00:18:11.064
So that's the high level intuition, but
if I were you I would certainly not

326
00:18:11.064 --> 00:18:14.749
be content with this somewhat hand wavy
explanation that I've given you so far.

327
00:18:14.749 --> 00:18:17.619
What I've told you is sort
of the hope the dream,

328
00:18:17.619 --> 00:18:20.740
why there is at least
a chance this might work.

329
00:18:20.740 --> 00:18:22.550
But the question remains, and

330
00:18:22.550 --> 00:18:26.490
I would encourage such skepticism,
which is does this really work?

331
00:18:27.720 --> 00:18:31.040
And to answer that we're going to have to
do some actual mathematical analysis, and

332
00:18:31.040 --> 00:18:31.760
that's what I'm going to show you.

333
00:18:31.760 --> 00:18:35.230
I'm going to show you a complete rigorous
analysis of the quick sort algorithm with

334
00:18:35.230 --> 00:18:39.550
random pivots, and we'll show that
yes in fact, it does really work.

335
00:18:39.550 --> 00:18:42.466
And this highlights what's going to be
a recurring them of this course, and

336
00:18:42.466 --> 00:18:45.065
a recurring theme in the study and
understanding of algorithms.

337
00:18:45.065 --> 00:18:48.902
Which is that quite often there's some
fundamental problem you're trying to code

338
00:18:48.902 --> 00:18:52.349
with a solution, you come up with
a novel idea, it might be brilliant, and

339
00:18:52.349 --> 00:18:53.480
it might suck.

340
00:18:53.480 --> 00:18:54.190
You have no idea.

341
00:18:54.190 --> 00:18:57.810
Now, obviously, you code up the idea,
run it on some concrete instances and

342
00:18:57.810 --> 00:19:00.120
get a feel for
whether it seems like a good idea or not.

343
00:19:00.120 --> 00:19:03.670
But if you really want to know
fundamentally what makes the idea good or

344
00:19:03.670 --> 00:19:05.620
what makes the idea bad, really,

345
00:19:05.620 --> 00:19:09.370
you need to turn to mathematical analysis
to give you a complete explanation.

346
00:19:09.370 --> 00:19:11.260
And that's exactly what we're
going to do with QuickSort,

347
00:19:11.260 --> 00:19:14.720
and then we'll explain in a very
deep way why it works so well.

348
00:19:16.460 --> 00:19:20.780
Specifically in the next sequence of three
videos I'm going to show you an analysis,

349
00:19:20.780 --> 00:19:23.235
a proof of the following
theorem about QuickSort.

350
00:19:25.250 --> 00:19:29.570
So under no assumptions about the data,
that is, for

351
00:19:29.570 --> 00:19:34.754
every input array of a given length,
say n, the average running

352
00:19:34.754 --> 00:19:40.239
time of QuickSort implemented with
random pivots is big O of n log n.

353
00:19:40.239 --> 00:19:42.177
And again in fact it's
theta of n log n but

354
00:19:42.177 --> 00:19:44.300
we'll just focus on
the big O of n log n part.

355
00:19:45.430 --> 00:19:50.330
So this is a very, very cool theorem about
this randomized QuickSort algorithm.

356
00:19:50.330 --> 00:19:55.090
One thing I want to be clear so that
you don't under sell this guarantee in

357
00:19:55.090 --> 00:19:59.710
your own mind, this is a worst case
guarantee with respect to the input.

358
00:19:59.710 --> 00:20:02.137
Okay so notice at the beginning
of this theorem what do we say?

359
00:20:02.137 --> 00:20:06.700
For every input array of length n,
all right?

360
00:20:06.700 --> 00:20:08.990
So, we have absolutely no
assumptions about the data.

361
00:20:08.990 --> 00:20:12.980
This is a totally general purpose
sorting separating which you can

362
00:20:12.980 --> 00:20:16.420
use whatever you want even if you have
no idea where the data is coming from.

363
00:20:16.420 --> 00:20:19.226
And these guarantees
are still going to be true.

364
00:20:20.630 --> 00:20:23.933
This of course is something I
held forth about at some length

365
00:20:23.933 --> 00:20:28.622
back in our guiding principles video, when
I argued that if you can get away with it,

366
00:20:28.622 --> 00:20:32.020
what you really want is
general purpose algorithms.

367
00:20:32.020 --> 00:20:34.410
Which make no data assumption,
so they can be used over and

368
00:20:34.410 --> 00:20:37.350
over again in all kinds
of different contexts and

369
00:20:37.350 --> 00:20:40.110
that still have great guarantees,
QuickSort is one of those.

370
00:20:40.110 --> 00:20:43.910
So basically if you have a data set and it
fits in the main memory of your machine,

371
00:20:43.910 --> 00:20:47.760
again sorting is a four free sub
routine in particular QuickSort.

372
00:20:47.760 --> 00:20:49.540
The QuickSort implementation is for free.

373
00:20:49.540 --> 00:20:52.650
So this just runs so blazingly fast,
doesn't matter what the array is,

374
00:20:52.650 --> 00:20:54.680
maybe you don't even know
why you want to sort it.

375
00:20:54.680 --> 00:20:56.020
But go ahead, why not?

376
00:20:56.020 --> 00:20:58.930
Maybe it will make your life easier,
like it did for example

377
00:20:58.930 --> 00:21:02.560
in the closest pair algorithm for those of
you who watch those two optional videos.

378
00:21:04.120 --> 00:21:08.180
Now the word average does
appear in this theorem and

379
00:21:08.180 --> 00:21:11.390
as I've been harping on, this average is
not over any assumptions on the data.

380
00:21:11.390 --> 00:21:14.390
We're certainly not assuming the the input
array is random in any sense.

381
00:21:14.390 --> 00:21:17.630
The input array can be anything, so
where is the average then coming from?

382
00:21:17.630 --> 00:21:21.390
The averaging is coming only from
randomness which is internal to

383
00:21:21.390 --> 00:21:22.380
our algorithm.

384
00:21:22.380 --> 00:21:26.360
Randomness that we put in the code in
ourselves, that we're responsible for.

385
00:21:28.690 --> 00:21:32.060
So remember, randomized algorithms have
the interesting property that even if you

386
00:21:32.060 --> 00:21:34.250
run it on the same input over and
over again,

387
00:21:34.250 --> 00:21:35.810
you're going to get different executions.

388
00:21:35.810 --> 00:21:38.830
So the running time of
a randomized algorithm can vary

389
00:21:38.830 --> 00:21:41.680
as you run it on the same input over and
over again.

390
00:21:41.680 --> 00:21:45.740
The quizzes have taught us that the
running time of QuickSort on a given input

391
00:21:45.740 --> 00:21:48.600
fluctuates from anywhere
between the best case of n log

392
00:21:48.600 --> 00:21:50.458
n to the worst case of n squared.

393
00:21:50.458 --> 00:21:54.429
So what this theorem is telling us is
that for every possible input array,

394
00:21:54.429 --> 00:21:57.760
while the running time does
indeed fluctuate between an upper

395
00:21:57.760 --> 00:22:00.780
bound of n squared and
a lower bound of n log n.

396
00:22:00.780 --> 00:22:02.820
The best case is dominating.

397
00:22:02.820 --> 00:22:07.230
On average it's n log n, on average
it's almost as good as the best case.

398
00:22:07.230 --> 00:22:09.180
That's what's so amazing about QuickSort.

399
00:22:09.180 --> 00:22:13.220
Those n squared that can pop up
once in a while, doesn't matter.

400
00:22:13.220 --> 00:22:14.340
You're never going to see it,

401
00:22:14.340 --> 00:22:17.370
you're always going to see this n log n
like behavior in randomized QuickSort.

402
00:22:17.370 --> 00:22:20.450
So for some of you I'll see you next
in a video on probability review,

403
00:22:20.450 --> 00:22:21.100
that's optional.

404
00:22:21.100 --> 00:22:23.880
For the rest of you I'll see you
in the analysis of this theorem.